---
title: "Data Visualization Group Project"
author: "Study Group 5: Etra Bianco, Vasu Dev Puri, Himanshu Singh, Yi Yan Ng, Mengze Sun, Christian Gonder"
date: "`r Sys.Date()`"
output: 
    html_document:
      number_sections: true
      highlight: haddock
      theme: spacelab
      toc: yes
      toc_depth: 2
      toc_float:
        collapsed: false
      fontzize: 10pt
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(cluster)
library(Hmisc)
library(factoextra)
library(purrr)
library(gridExtra)
library(data.table)
library(reshape2)
library(ggplot2)  
library(dplyr)
library(readxl)
library(kableExtra)
library(janitor)
library(ggrepel)
library(scales)
library(rnaturalearth)
library(countrycode)
library(sf)
library(hrbrthemes)
library(pracma)
```

**Stock Market Analysis**

Our agenda is to portray the impact COVID-19 has had on financial markets in 2020, visualizing the performance of different stock market indeces (SP500, FTSE100, HSI…) throughout the year and identifying the most significant shocks as well as drawing parallels with the events and world news that might have been the cause.

We intend also to conduct an analysis across geographies by creating a geospatial map of major indices around the world and compare the performance in those markets using a heat map.

Furthermore, we intend to analyze the performance across industries and ascertain sectors that were counter-cyclical / robust. 

Additionally, we intend to compare the above to the market’s performance during the 2008 economic crisis and their respective revivals.

For the purpose of this visualization we use Financial market data sourced from Capital IQ.

(1)	A statement of the question or purpose. What problems or questions did you set out to analyse? What were the key issues raised?

#Loading and Investigating the data

```{r ,message = FALSE , warning=FALSE}

#Loading the data 

indeces <- read_excel("Data/indices_data.xls")


```






```{r eval = FALSE}

#Having a first look at what the data includes
head(indeces)
#Using the summary functions to have a more general idea about what is in the data
glimpse(indeces) 
#Using the describe function for more detailed information about data.
describe(indeces) 

```

The data file “Charting Excel Export - Nov 21st 2020 11_15_53 am.xls” contains information extracted from the Capital IQ library that outlines the performance at the close of day of different stock market indeces (SP500, FTSE100, HSI…) from 2007 till today. 

# Cleaning the data 

To clean the data we need to check:
- duplicates
- missing values (NAs)
- empty values
- cross column consistency 

### Cleaning column names
We first shorten the names of the columns in our data frame. Moreover, we run the clean names function to remove capital letters and add underscores to separate words. 
```{r clean names, message = FALSE , warning=FALSE}

#Copying the data into a new data frame we can process
originalData_process <- indeces


#we shorten the names of our variables (columns)
originalData_process <- originalData_process %>%
  rename(c("FTSE100" = 'FTSE 100 Index (^UKX) - Index Value', "S&P500" = 'S&P 500 (^SPX) - Index Value',"BSE Sensex" = "S&P BSE Sensex (^SENSEX) - Index Value",  "DAX" = 'Germany DAX Index (Performance) (^DAX) - Index Value'), "SSE" = "Shanghai Stock Exchange Composite Index (^000001) - Index Value", "TOPIX" = "TOPIX INDEX (^TOPIX) - Index Value","PX" = "Paris CAC 40 Index (^PX1) - Index Value","Euronext" = "Euronext 150 Index - Index Value")

#cleaning the variable names
originalData_process<-clean_names(originalData_process)  

```


### Initial data cleaning

Before we start looking at the contents of the data we remove empty columns and rows and want to remove duplicates if any.

```{r remove_empty ,message = FALSE , warning=FALSE}
#Removing empty columns and rows
originalData_process<-remove_empty(originalData_process, which = c("rows","cols"))


#Checking for duplicates
dupes<-originalData_process%>%get_dupes(dates)

```

### Missing data 

As evident from the below, we have several missing values across the different indexes throughout our data frame, however, for the most part, these are on days where the stock exchange is closed eg. New Years (01/01) and other bank holidays and hence we can ignore them and proceed. 


```{r check for na, message=FALSE}
#chekcing for nas accross the columns
nas <- sapply(originalData_process , function(x) sum(is.na(x)))
data.frame(nas) %>% 
  arrange(desc(nas)) %>% 
  rename('Number of NAa' = 'nas') %>% 
  kbl() %>% 
  kable_styling()

```




#Plot1:  Map for covid  

```{r, message = FALSE , warning=FALSE}

#extracting populatuion
library(wpp2019)

#get 2020 pop and country code
data(pop)
pop_2020 <- pop %>% 
  select(c(1,17)) %>% 
  mutate(`2020` = `2020` * 1000)

#importing covid and cleaning data set 
covid_1 <- read_csv('data/covid_jh.csv')
covid_1<- clean_names(covid_1)

#extracting only the final columns
covid <- covid_1[-1 ,c(319:320)] %>% 
  rename('iso_a3' = 'iso_3166_1_alpha_3_codes') %>% 
  group_by(iso_a3) %>% 
  summarise(total = sum(x12_1_20))



#getting geometrty and isocodes 
earth <- ne_countries(scale = 50, returnclass = "sf") %>%
  select(admin, geometry, iso_a3)



#getting geometrty and isocodes 
iso_un <- ne_countries(scale = 50, returnclass = "sf") %>% 
   select(iso_a3  , un_a3 , iso_n3  ) %>% 
   mutate(un_a3 = as.integer(un_a3) , iso_n3 = as.integer(iso_n3))
 
 
 
#joinning 2020 pop and country code with geometry together
 map_100k <- left_join(pop_2020, iso_un , by = c('country_code' = 'iso_n3')) %>% 
   filter(is.na(iso_a3) == FALSE) %>%
   select(`2020`, iso_a3, geometry) %>% 
   left_join(covid %>% 
   select(iso_a3 , total) , by = 'iso_a3') %>% 
  mutate(cases_per100 = (total/`2020`)*1e5) 
 
 
 
#converting to geographic data base
map_100k <- map_100k %>% 
  sf::st_as_sf() 
  

options(scipen = 999)

#making map showing cases per 100K population worldwide
map_100k  %>%
  ggplot() +
  geom_sf(aes(fill = cases_per100), size = 0.1, color = "white") +
  coord_sf(crs = "+proj=longlat" , datum = NA) + 
  scale_fill_distiller( palette = "Blues", 
                    direction = 1 , 
                    guide = guide_colorbar(
                      title = "Number of cases per 100k",
                      title.hjust = 0.5,
                      labels = unit_format(unit = "K", scale = 1e-3) , 
                      direction = "vertical",
                      nrow = 1,
                      keyheight = unit(3, units = "mm"),
                      keywidth = unit(3, units = "mm"),
                      title.position = "top"  ,
                      barheight =  10 , barwidth = 1.5)) +
  theme_minimal() +
  theme(legend.direction = "horizontal",
        legend.position = "right" , 
        panel.grid.major.y = element_blank() , panel.grid.minor.y = element_blank(),  plot.title = element_text(size = 15, face= "bold"), plot.subtitle = element_text(size = 13)) +
  labs(title = "Covid Map",
       subtitle = "Covid Cases Worldwide per 100k population")



```

#Plot2: Timeline of all the indices 

```{r ,message = FALSE , warning=FALSE}
### USA , ftse , germany during covid 
#limit data to 2020 jan, feb march 
early_covid <- originalData_process %>% 
  filter(year(dates) == 2020 )

#removing first row which is new years 
early_covid <- early_covid[-1,]


#transforming each column to a percentage change from initial value 
for (i in c(2:9)){
inital_val <- unlist(early_covid[1,i[[1]]])
early_covid[,i] <- (early_covid[ ,i]/inital_val)- 1

}

#creating data frame for cross correlation
ccf_df <- early_covid %>% 
  select(`sse`, `ftse100`, `s_p500`, `euronext`, `bse_sensex` )




#removing any nas
early_covid <- early_covid %>% 
  pivot_longer( cols = c(`sse`, `ftse100`, `s_p500`, `euronext`, `bse_sensex` ), names_to = 'market' , values_to = 'pct_change') %>% 
  filter(is.na(pct_change) == FALSE) 
  
  
# plot 
plot_3 <- ggplot(early_covid) +
  geom_line(aes(x = dates, y=pct_change, color = market) , size = .8 , alpha = 0.5) + 
   geom_rect(aes(xmin= as.POSIXct(as.Date(c("2020-03-19"))) , xmax  = as.POSIXct(as.Date(c("2020-06-13"))),
         ymin = -0.4 , ymax = 0.1) , alpha = 0.002) +
  geom_hline(yintercept = 0 , size = 0.6 ) + 
  scale_y_continuous(labels =  scales::percent) + 
  theme_minimal() + 
  theme(panel.grid.major.x =  element_blank() , 
        panel.grid.minor =  element_blank() , 
        panel.grid.major.y = element_line(size = 1),
        legend.position = 'bottom') + 
  labs(y = 'Cumulative percentage change' , x = '' ) +
  theme(plot.title = element_text(size = 15, face= "bold"), plot.subtitle = element_text(size = 13)) +
  labs(title = "Stock market reaction to the Covid19 Pandemic",
       subtitle = "Performance of major indeces in 2020" , color = 'Index')+
  
  #adding annotation of the main time of the epidemic
  annotate("text", x = as.POSIXct(as.Date(c("2020-01-10","2020-02-14","2020-05-09","2020-10-01", "2020-11-21"))), 
           y = c(-0.42, -0.42,-0.42,-0.42,-0.42), 
           label = c("First Lockdown \n China", "Europe \n 1st Wave","USA Lockdown","Europe \n 2nd Wave", "Vaccine \n Announced"), 
           color="black", size=3 , angle=0)



plot_3


```


## Cross Correlation Matrix


```{r , message = FALSE , warning=FALSE}

#correlation matrix for the various indices.
cols_1 = character()
cols_2 = character()
corr_list = double()


#creating  a data framw wiht correlatin scores 
i = 1
for (col1 in colnames(ccf_df)){
  j = 1
  for (col2 in colnames(ccf_df)){
    
 #calculating the cross correlation for the two columns    
ccf_values = ccf(ccf_df[rowSums(is.na(ccf_df[, i]))  == 0,i] , ccf_df[rowSums(is.na(ccf_df[, j]))  == 0,j] , plot = FALSE)

#calculating the values
values = ccf_values[0][[1]][[1]]

#appending the values 
cols_1[[length(cols_1) + 1]] = col1
cols_2[[length(cols_2) + 1]] = col2
corr_list[[length(corr_list) + 1]] = values

j = j+1 
  }
i = i +1 
}
```


```{r}
#making correlation heatmap
plot_4 <-data.frame(cbind(cols_1 ,cols_2 , corr_list))%>% 
  ggplot() + 
    geom_tile(aes(x  = cols_1 , y = cols_2 , fill = as.double(corr_list))) + 
   scale_fill_distiller(palette = "Blues" , 
                        direction = 1) +
  labs(x = '' , y = ' '  ) + 
  guides(fill =  guide_colourbar(title = 'Correlation' , barheight = 15 , barwidth = 1))+
  #adjusting the theme
  theme_ipsum() + 
    theme(panel.grid.minor = element_blank() , panel.grid.major = element_blank()) +
  theme( axis.line = element_line(), panel.grid.major = element_blank() , panel.grid.minor= element_blank(),  
         plot.title = element_text(size = 15, face= "bold"), plot.subtitle = element_text(size = 12) , 
         ) +
  labs(
       subtitle = "Cross Correlation for the various indices in 2020 ")

plot_4
```


# plot 3  volatility vs drop wiht size of the stock market being the size of the circle/the total value lost 


```{r , fig.width= 12, fig.height= 8}
# loading original dataframe to a new one to make graphs
bubble_df <- originalData_process 

# identifying dataframe to make bubble plot
 plot_df2 <- bubble_df %>%
  filter(month(dates) %in% c(1,2,3)) %>%
  filter(year(dates) == 2020) %>%
  pivot_longer(cols = c(2:9),  names_to = 'market' , values_to = 'price' ) %>% 
  filter(is.na(price) == FALSE) %>% 
  group_by(market) %>% 
   # We use the following calculation method to show volatity
  mutate(per_change = (price/lag(price))  - 1) %>% 
  summarise(.groups = "keep" , first_val = first(price) , last_val = last(price) , std = sd(per_change , na.rm = TRUE)) %>% 
  mutate(perc_drop =(last_val/first_val ) - 1) %>% 
  select(market, perc_drop , std , last_val) 

# adding covid cases to illustrate the size of bubbles
  plot_df2$covidcases =c(685.708, 1306.549, 3740.9595, 2420.36, 3403.425, 4145.376, 6.01331, 119.37083 )

glimpse(plot_df2)

# making the plot showing Volatility vs The Percentage Drop in Index Value.
# the size of the bubble means the number of cases.
plot_2 <- plot_df2 %>% 
  ggplot() +
  geom_point(aes(x = perc_drop , y = std , size = covidcases,  color = market) , alpha = 0.5) + 
  theme_minimal() + 
  scale_size(range = c(6,25)) + 
  # geom_label_repel(mapping = aes(x = perc_drop , y = std ,  label = market) , 
  #                  force = 20 , direction = "both" , face = 'bold' , )+
  # geom_label(mapping = aes(x = perc_drop , y = std ,  label = market , fill = market) 
  #            , direction = "both" , face = 'bold' ,nudge_x = 0,
  # nudge_y = 0.003, )+
   geom_label_repel(mapping = aes(x = perc_drop , y = std ,  label = market ) 
             , direction = "both" , face = 'bold' ,nudge_x = 0,
  nudge_y = 0.003, )+
  scale_x_continuous(labels = scales::percent ,limits = c(-0.32, -0.1) ) + 
  scale_y_continuous(labels = scales::percent,limits = c(0.015, 0.04)) + 
  theme(legend.position =  'bottom') + 
   guides(color=guide_legend("market"), fill = FALSE) +
  theme_minimal() +
  theme( axis.line = element_line(), panel.grid.major = element_blank() , panel.grid.minor= element_blank(),  
         plot.title = element_text(size = 15, face= "bold"), plot.subtitle = element_text(size = 12) , 
         ) +
  labs(title = "Stonger financial markets for countries that kept Covid under control",
       subtitle = "Volatility vs The Percentage Drop in Index Value ", 
       y = "Volatility (index standard deviation)", x = "Percentage drop" , 
       size = 'Covid Cases per 100K')  +guides(colour=FALSE)
  

plot_2

```




# Plot 4:comparing negative quarters for each market. 

```{r ,message = FALSE , warning=FALSE}
library(scales)
library(zoo)
#creating the percentage drops for each quarters 
quarters <- originalData_process %>%
  mutate(quarter = quarter(dates) , year = year(dates) , q_y = paste0(quarter, '/' , year)) %>% 
pivot_longer( cols = c(`sse`, `ftse100`, `s_p500`, `euronext`, `bse_sensex` , `topix`), names_to = 'market' , values_to = 'price')%>%
  filter(is.na(price) == FALSE) %>% 
  #groupby quarter and market 
  group_by(year, q_y, market) %>% 
  #finding first and last value 
  summarise(first_val = first(price)  , last_val = last(price) , first_day = min(as.Date(dates))) %>% 
  #calculating the percentage drop
  mutate(perc = (last_val/first_val)  - 1)%>% 
  #keeping only negative
  filter(perc < -0.08) 

df <- quarters %>% 
  pivot_wider(id_cols = c(`q_y` , `market`, `perc`) , names_from = 'market' , values_from= 'perc')



#creating a top 10 quarters for each index 
df_new <- data.frame()
for (col in colnames(df[,-1])) {
print(col)
temp_df <- df[is.na(df[,col]) == FALSE, col]
#extract covid
last_val <- last(temp_df)
#get second last value
i_final <- nrow(temp_df)-1
#remove covid vaue 
temp_df <- temp_df[1: i_final ,]
#sort by top 9
temp_df <- temp_df[order(temp_df[col] , decreasing =  FALSE)[1:9] , ]
#add covid
temp_df[10, col] <- last_val

df_new <- append(temp_df , df_new)
}


df_new$index <- c(1:10)

df_new$fill_box <- c('no_color', 'no_color', 'no_color', 'no_color' ,'no_color' , 'no_color' ,'no_color', 'no_color' , 'no_color' , 'COVID')



#plotting bar chart 
plot_1 <- data.frame(df_new) %>%
  pivot_longer(c(1:6) , names_to = 'market' , values_to  = 'perc' ) %>% 
  ggplot() + 
  geom_col(aes(x = index, y = perc, fill = fill_box))+
  facet_wrap(.~market)+
theme_minimal() +
  geom_hline(yintercept = 0)+
  scale_fill_manual(values=c("#04449c","light grey")) +
  theme(panel.grid.minor = element_blank() ,
        panel.grid.major.x  = element_blank(), 
        legend.position = 'bottom' , axis.ticks.x = element_blank()) + 
  scale_y_continuous(label = scales::percent)  + 
  labs (title = "Covid sparked an alarming drop in Q1 of 2020",  
  subtitle = "10 worst performing quarters per index [COVID Quarter in Blue]",
       x = "",
  y = "Percentage change") + 
    theme(plot.title = element_text(size = 15, face= "bold"), plot.subtitle = element_text(size = 13) ,
          axis.ticks.x = element_blank() , axis.text.x =  element_blank()) + 
  guides(fill = FALSE)


plot_1
  
```


# plot 6 the aggregate performance throughout the year [average performance for the full year]

```{r , message = FALSE , warning=FALSE}
#chekcing for total number rows wiht na values 
originalData_process[rowSums(is.na(originalData_process[, -1])) > 0,]
#total of 11114 rows which is ~20% so could use the prevous datts
originalData_process[rowSums(is.na(originalData_process[, -1])) > 0 ,] %>% 
  filter(year(dates) == 2020)
#for 2020 reduced to 50 which is 50/240 
full_year_2020 <- originalData_process[!rowSums(is.na(originalData_process[, -1])) > 0 ,] %>% 
  filter(year(dates) == 2020) 

#transforming each column to a percentage change from initial value 
for (i in c(2:9)){
inital_val <- unlist(full_year_2020[1,i[[1]]])
full_year_2020[,i] <- (full_year_2020[ ,i]/inital_val)- 1

}


#getting the avereage perforrmance of all indices 
full_year_2020$world_aggregate <- rowSums(full_year_2020[, -1])/8


#plotting the graph 
plot_5 <- full_year_2020 %>% 
  select(dates, s_p500 , world_aggregate) %>% 
  pivot_longer(cols = 2:3 , names_to ='market' , values_to = 'pct_change' ) %>% 
  ggplot() +
  geom_line(aes(x = dates, y=pct_change, color = market) , size = 1.5 , alpha = 0.5) + 
  geom_hline(yintercept = 0 , size = 1 ) + 
  scale_y_continuous(labels =  scales::percent) + 
  scale_color_manual(values = c('grey' , '#04449c'))+
  theme_minimal() + 
  theme(panel.grid.major.x =  element_blank() , 
        panel.grid.minor =  element_blank() , 
        panel.grid.major.y = element_line(size = 1),
        legend.position = 'bottom') + 
  labs (subtitle = "Indices worldwide in 2020 compared to the S&P500 performance",  
  title = "Financial Markets are on average aligned to the S&P500",
       x = "time",
  y = "indeces" , 
  color = 'Index') + 
    theme(panel.background =element_blank(),
         panel.grid.major  = element_blank() , 
         panel.grid.minor = element_blank(),
         axis.ticks.x = element_blank(), 
         axis.ticks.y = element_blank(), 
         plot.title = element_text(size = 13, face= "bold"), plot.subtitle = element_text(size = 13))
plot_5

```


# Add images 



